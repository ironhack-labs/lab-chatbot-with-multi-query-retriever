{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with LangChain, OpenAI, and MultiQuery Retriever\n",
    "\n",
    "This interactive workbook demonstrates example of Elasticsearch's [MultiQuery Retriever](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html) to generate similar queries for a given user input and apply all queries to retrieve a larger set of relevant documents from a vectorstore.\n",
    "\n",
    "Before we begin, we first split the fictional workplace documents into passages with `langchain` and uses OpenAI to transform these passages into embeddings and then store these into Elasticsearch.\n",
    "\n",
    "We will then ask a question, generate similar questions using langchain and OpenAI, retrieve relevant passages from the vector store, and use langchain and OpenAI again to provide a summary for the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install -q langchain-elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install -qU jq lark langchain langchain_openai tiktoken\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Elasticsearch\n",
    "\n",
    "ℹ️ We're using an Elastic Cloud deployment of Elasticsearch for this notebook. If you don't have an Elastic Cloud deployment, sign up [here](https://cloud.elastic.co/registration?utm_source=github&utm_content=elasticsearch-labs-notebook) for a free trial. \n",
    "\n",
    "We'll use the **Cloud ID** to identify our deployment, because we are using Elastic Cloud deployment. To find the Cloud ID for your deployment, go to https://cloud.elastic.co/deployments and select your deployment.\n",
    "\n",
    "We will use [ElasticsearchStore](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html) to connect to our elastic cloud deployment, This would help create and index data easily.  We would also send list of documents that we created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearch connection established and vector store created!\n"
     ]
    }
   ],
   "source": [
    "# Retrieve API keys securely\n",
    "ELASTIC_CLOUD_ID = getpass(\"Enter your Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Enter your Elastic API Key: \")\n",
    "OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Connect to Elasticsearch using ElasticsearchStore\n",
    "vectorstore = ElasticsearchStore(\n",
    "    es_cloud_id=ELASTIC_CLOUD_ID,  # Elastic Cloud ID\n",
    "    es_api_key=ELASTIC_API_KEY,    # Elastic API key\n",
    "    index_name=\"reviews_sentiment_cluster_nlp\",  # Name of the index where data will be stored\n",
    "    embedding=embeddings  # Use OpenAI embeddings for text\n",
    ")\n",
    "\n",
    "# Optionally, print to confirm that the vector store has been initialized\n",
    "print(\"Elasticsearch connection established and vector store created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Data into Elasticsearch\n",
    "Let's download the sample dataset and deserialize the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/example-apps/chatbot-rag-app/data/data.json\"\n",
    "\n",
    "response = urlopen(url)\n",
    "data = json.load(response)\n",
    "\n",
    "with open(\"temp.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Documents into Passages\n",
    "\n",
    "We’ll chunk documents into passages in order to improve the retrieval specificity and to ensure that we can provide multiple passages within the context window of the final question answering prompt.\n",
    "\n",
    "Here we are chunking documents into 800 token passages with an overlap of 400 tokens.\n",
    "\n",
    "Here we are using a simple splitter but Langchain offers more advanced splitters to reduce the chance of context being lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jq\n",
      "  Downloading jq-1.8.0-cp311-cp311-win_amd64.whl.metadata (7.2 kB)\n",
      "Downloading jq-1.8.0-cp311-cp311-win_amd64.whl (416 kB)\n",
      "Installing collected packages: jq\n",
      "Successfully installed jq-1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" pip install jq \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document chunks created: 15\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define a function to populate metadata\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    # Example: Populate metadata fields\n",
    "    metadata[\"name\"] = record.get(\"name\", \"\")\n",
    "    metadata[\"summary\"] = record.get(\"summary\", \"\")\n",
    "    metadata[\"url\"] = record.get(\"url\", \"\")\n",
    "    metadata[\"category\"] = record.get(\"category\", \"\")\n",
    "    metadata[\"updated_at\"] = record.get(\"updated_at\", \"\")\n",
    "    return metadata\n",
    "\n",
    "# Load documents from JSON file\n",
    "loader = JSONLoader(\n",
    "    file_path=\"temp.json\",  # Path to your JSON file\n",
    "    jq_schema=\".[]\",  # jq schema to extract records\n",
    "    content_key=\"content\",  # Key in JSON for the main content\n",
    "    metadata_func=metadata_func  # Function to populate metadata\n",
    ")\n",
    "\n",
    "# Split documents into 800 token passages with 400 token overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=800,  # Chunk size: 800 tokens\n",
    "    chunk_overlap=400  # Overlap: 400 tokens between chunks\n",
    ")\n",
    "\n",
    "# Load and split the documents\n",
    "docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "# Check the number of documents created and verify the chunks\n",
    "print(f\"Number of document chunks created: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Import Passages\n",
    "\n",
    "Now that we have split each document into the chunk size of 800, we will now index data to elasticsearch using [ElasticsearchStore.from_documents](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html#langchain.vectorstores.elasticsearch.ElasticsearchStore.from_documents).\n",
    "\n",
    "We will use Cloud ID, Password and Index name values set in the `Create cloud deployment` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error adding texts: 10 document(s) failed to index.\n",
      "First error reason: [1:4589] failed to parse field [metadata.updated_at] of type [date] in document with id 'dcc327cf-58c2-4cdd-9f0c-2db7552c4309'. Preview of field's value: ''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BulkIndexError: 10 document(s) failed to index.\n",
      "10 documents failed to index.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import ElasticsearchStore\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from getpass import getpass\n",
    "from elasticsearch.helpers import BulkIndexError\n",
    "\n",
    "# Step 1: Load the JSON file and metadata\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"name\"] = record.get(\"name\", \"\")\n",
    "    metadata[\"summary\"] = record.get(\"summary\", \"\")\n",
    "    metadata[\"url\"] = record.get(\"url\", \"\")\n",
    "    metadata[\"category\"] = record.get(\"category\", \"\")\n",
    "    metadata[\"updated_at\"] = record.get(\"updated_at\", \"\")\n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=\"temp.json\",\n",
    "    jq_schema=\".[]\",\n",
    "    content_key=\"content\",\n",
    "    metadata_func=metadata_func\n",
    ")\n",
    "\n",
    "# Step 2: Split documents into passages\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400\n",
    ")\n",
    "\n",
    "# Load and split documents\n",
    "docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "# Step 3: Set up Elasticsearch connection and embedding model\n",
    "ELASTIC_CLOUD_ID = getpass(\"Enter your Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Enter your Elasticsearch API Key: \")\n",
    "OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Step 4: Index the documents into Elasticsearch with error handling\n",
    "try:\n",
    "    vectorstore = ElasticsearchStore.from_documents(\n",
    "        docs,  # Pass the documents list (split passages) here\n",
    "        embedding=embeddings,  # Ensure your embedding model is initialized\n",
    "        es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "        es_api_key=ELASTIC_API_KEY,\n",
    "        index_name=\"reviews_sentiment_cluster_nlp\"\n",
    "    )\n",
    "    print(\"Documents indexed successfully!\")\n",
    "\n",
    "except BulkIndexError as e:\n",
    "    # Catch bulk indexing errors and log details\n",
    "    print(f\"BulkIndexError: {e}\")\n",
    "    failed_docs = e.errors\n",
    "    print(f\"{len(failed_docs)} documents failed to index.\")\n",
    "\n",
    "except Exception as ex:\n",
    "    # Catch any other exceptions\n",
    "    print(f\"An error occurred: {ex}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with MultiQuery Retriever\n",
    "\n",
    "Now that we have the passages stored in Elasticsearch, we can now ask a question to get the relevant passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiQueryRetriever\n",
    "from langchain.vectorstores import ElasticsearchStore\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from getpass import getpass\n",
    "\n",
    "# Initialize embeddings\n",
    "OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Set up Elasticsearch connection\n",
    "ELASTIC_CLOUD_ID = getpass(\"Enter your Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Enter your Elasticsearch API Key: \")\n",
    "\n",
    "# Initialize ElasticsearchStore to connect to your Elasticsearch index\n",
    "vectorstore = ElasticsearchStore(\n",
    "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "    es_api_key=ELASTIC_API_KEY,\n",
    "    index_name=\"reviews_sentiment_cluster_nlp\",  # Ensure you use the correct index name\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Initialize the MultiQueryRetriever\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(vectorstore.as_retriever(), llm)\n",
    "\n",
    "# Now you can use 'retriever' in your context or question-answering pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgetpass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getpass\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Step 3: Set up Elasticsearch connection and embedding model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m ELASTIC_CLOUD_ID \u001b[38;5;241m=\u001b[39m \u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter your Elastic Cloud ID: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m ELASTIC_API_KEY \u001b[38;5;241m=\u001b[39m getpass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your Elasticsearch API Key: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m OPENAI_API_KEY \u001b[38;5;241m=\u001b[39m getpass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your OpenAI API Key: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mktmi\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ipykernel\\kernelbase.py:1265\u001b[0m, in \u001b[0;36mKernel.getpass\u001b[1;34m(self, prompt, stream)\u001b[0m\n\u001b[0;32m   1258\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1261\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `stream` parameter of `getpass.getpass` will have no effect when using ipykernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1262\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1263\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1264\u001b[0m     )\n\u001b[1;32m-> 1265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mktmi\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Import the updated ElasticsearchStore from langchain_elasticsearch\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from getpass import getpass\n",
    "\n",
    "# Step 3: Set up Elasticsearch connection and embedding model\n",
    "ELASTIC_CLOUD_ID = getpass(\"Enter your Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Enter your Elasticsearch API Key: \")\n",
    "OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Use ElasticsearchStore from langchain_elasticsearch package\n",
    "vectorstore = ElasticsearchStore(\n",
    "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "    es_api_key=ELASTIC_API_KEY,\n",
    "    index_name=\"reviews_sentiment_cluster_nlp\",  # Ensure you use the correct index name\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"Documents indexed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide information on the sales team at NASA?', '2. How does the sales team operate within NASA?', '3. What are the responsibilities of the NASA sales team?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Answer ----\n",
      "I'm sorry, I don't know the answer to that question as it is not mentioned in the retrieved context. The context is about sales strategies and work policies for a tech company, not NASA.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import format_document\n",
    "from langchain.vectorstores import ElasticsearchStore\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from getpass import getpass\n",
    "import logging\n",
    "\n",
    "# Set up logging to monitor retriever activities\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "# Step 3: Set up Elasticsearch connection and embedding model\n",
    "ELASTIC_CLOUD_ID = getpass(\"Enter your Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Enter your Elasticsearch API Key: \")\n",
    "OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize ElasticsearchStore\n",
    "vectorstore = ElasticsearchStore(\n",
    "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "    es_api_key=ELASTIC_API_KEY,\n",
    "    index_name=\"reviews_sentiment_cluster_nlp\",  # Ensure you use the correct index name\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Initialize the MultiQueryRetriever\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(vectorstore.as_retriever(), llm)\n",
    "\n",
    "\n",
    "# Define the LLM context prompt template\n",
    "LLM_CONTEXT_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Be as verbose and educational in your response as possible.\n",
    "\n",
    "    context: {context}\n",
    "    Question: \"{question}\"\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Define the document prompt template for formatting documents\n",
    "LLM_DOCUMENT_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"---\n",
    "    SOURCE: {name}\n",
    "    {page_content}\n",
    "    ---\"\"\"\n",
    ")\n",
    "\n",
    "# Function to combine documents into a formatted string\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=LLM_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "# Define the retriever and parallel processing of context and question\n",
    "_context = RunnableParallel(\n",
    "    context=retriever | _combine_documents,  # Fetch and combine documents\n",
    "    question=RunnablePassthrough(),  # Pass the question as-is\n",
    ")\n",
    "\n",
    "# Create a chain that combines context, question, and LLM response\n",
    "chain = _context | LLM_CONTEXT_PROMPT | llm\n",
    "\n",
    "# Invoke the chain with a sample question\n",
    "ans = chain.invoke(\"what is the nasa sales team?\")\n",
    "\n",
    "# Output the answer\n",
    "print(\"---- Answer ----\")\n",
    "print(ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate at least two new iteratioins of the previous cells - Be creative.** Did you master Multi-\n",
    "Query Retriever concepts through this lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration 1: Enhanced Prompting with User Feedback\n",
    "In this iteration, we'll modify the prompt templates to gather user feedback after answering a question, which can help refine the model's understanding of what constitutes a good answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide information on the sales team at NASA?', '2. How does the sales team operate within NASA?', '3. What are the responsibilities of the NASA sales team?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Answer ----\n",
      "\n",
      "I'm sorry, I don't know the answer to that question. Would you like me to try and find more information for you?\n"
     ]
    }
   ],
   "source": [
    "# Iteration 1: Enhanced Prompting with User Feedback\n",
    "\n",
    "# Define an enhanced LLM context prompt template to include user feedback\n",
    "LLM_CONTEXT_PROMPT_FEEDBACK = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    Be as verbose and educational in your response as possible.\n",
    "\n",
    "    context: {context}\n",
    "    Question: \"{question}\"\n",
    "    Answer:\n",
    "\n",
    "    After answering, please ask the user for feedback: \n",
    "    \"Was this answer helpful? (yes/no)\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Update the context definition to utilize the new prompt\n",
    "_context_feedback = RunnableParallel(\n",
    "    context=retriever | _combine_documents,  # Fetch and combine documents\n",
    "    question=RunnablePassthrough(),  # Pass the question as-is\n",
    ")\n",
    "\n",
    "# Create a chain that combines context, question, and LLM response with feedback\n",
    "chain_feedback = _context_feedback | LLM_CONTEXT_PROMPT_FEEDBACK | llm\n",
    "\n",
    "# Invoke the chain with a sample question\n",
    "ans_feedback = chain_feedback.invoke(\"What is the NASA sales team?\")\n",
    "\n",
    "# Output the answer with user feedback request\n",
    "print(\"---- Answer ----\")\n",
    "print(ans_feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration 2: Contextualization with Related Questions\n",
    "In this iteration, we'll enhance the retriever to also provide related questions that could give the user more context or clarify their queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide information on the sales team at NASA?', '2. How does the sales team operate within NASA?', '3. What are the responsibilities of the NASA sales team?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Answer ----\n",
      "1. What is the sales team structure at our company?\n",
      "    2. How many members are on the sales team?\n",
      "    3. What are the responsibilities of the sales team?\n",
      "    4. How does the sales team contribute to the company's overall goals?\n",
      "    5. What training and support is provided to the sales team?\n",
      "    6. How is the sales team's performance evaluated?\n",
      "    7. What strategies does the sales team use to reach their objectives?\n",
      "    8. How does the sales team collaborate with other departments?\n",
      "    9. What is the sales team's role in customer satisfaction?\n",
      "    10. How does the sales team adapt to changes in the market?\n"
     ]
    }
   ],
   "source": [
    "# Iteration 2: Contextualization with Related Questions\n",
    "\n",
    "# Define an additional prompt template to include related questions\n",
    "RELATED_QUESTIONS_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question.\n",
    "    Additionally, suggest related questions that the user might find useful.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "\n",
    "    context: {context}\n",
    "    Question: \"{question}\"\n",
    "    Answer:\n",
    "    Related Questions:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Update the context definition to utilize the related questions prompt\n",
    "_context_related = RunnableParallel(\n",
    "    context=retriever | _combine_documents,  # Fetch and combine documents\n",
    "    question=RunnablePassthrough(),  # Pass the question as-is\n",
    ")\n",
    "\n",
    "# Create a chain that combines context, question, and LLM response with related questions\n",
    "chain_related = _context_related | RELATED_QUESTIONS_PROMPT | llm\n",
    "\n",
    "# Invoke the chain with a sample question\n",
    "ans_related = chain_related.invoke(\"What is the NASA sales team?\")\n",
    "\n",
    "# Output the answer along with related questions\n",
    "print(\"---- Answer ----\")\n",
    "print(ans_related)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
